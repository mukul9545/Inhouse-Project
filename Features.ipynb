{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad86fb1-11e6-4049-a3ff-5ba583fa6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Ensure the required NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize necessary objects\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('blinkit_tweets.csv')\n",
    "def remove_emojis(text):\n",
    "    # This function removes emojis from the text\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "additional_words_to_remove = set([\"blinkit\", \"zepto\", \"bigbasket\", \"swiggy instamart\", \"zomato\", \"swiggy\",\"swiggycares\",\"bigbasketcom\",\"dunzo\",\"jiomart\",\n",
    "                             \"instamart\" ,\"dmart\",\"big\", \"basket\",\"flipkart\"])\n",
    "\n",
    "def data_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https\\S+|www\\S+https\\S+\", '', text, flags=re.MULTILINE) # remove urls , hyperlinks\n",
    "    text = re.sub(r'@\\w+', ' ', text)  # Replace mentions with whitespace\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation, including apostrophes \n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = remove_emojis(text)  # Remove emojis\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter out tokens that are stop words\n",
    "    text_tokens = [w for w in text_tokens if w not in stop_words]\n",
    "    \n",
    "    # Filter out tokens that contain any of the additional words to remove as substrings\n",
    "    text_tokens = [w for w in text_tokens if not any(word in w for word in additional_words_to_remove)]\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    return \" \".join(text_tokens)\n",
    "\n",
    "df['processed_text']=df['text'].apply(data_processing)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatization(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df['processed_text'] = df['processed_text'].apply(lemmatization)\n",
    "\n",
    "f = df.dropna(subset=['processed_text'])\n",
    "df = df[df['processed_text'].str.strip().astype(bool)]\n",
    "\n",
    "# Expanded keywords dictionary\n",
    "keywords = {\n",
    "    'customer_support': ['customer support', 'support', 'help', 'assistance', 'customer care', 'complaint', 'resolution', 'query', 'live chat', 'support team', 'call center', 'issue','call','money','refund','reply','response','resolve','resolved','email'],\n",
    "    'delivery_services': ['deliver','hour','minute','delivery','cancel', 'ship', 'arrive', 'courier', 'fast delivery', 'late delivery', 'on-time delivery', 'delivery time', 'delivery service', 'express delivery', 'next day delivery', 'same day delivery', 'contactless delivery','time','receive','quick'],\n",
    "    'product_quality': ['quality', 'defective', 'satisfied', 'unsatisfied', 'poor', 'excellent', 'high quality', 'low quality', 'durable', 'broken', 'faulty', 'fresh', 'rotten', 'expiry', 'damaged', 'premium quality', 'organic', 'branded','like','dislike','product','return','good quality','bad quality','miss','spolied','spoilt','expire','cold'],\n",
    "    'discounts': ['discount', 'offer', 'sale', 'promotion', 'deal', 'coupon', 'voucher', 'savings', 'markdown', 'cashback', 'special offer', 'buy one get one', 'limited time offer','price','free','mrp']\n",
    "}\n",
    "\n",
    "# Function to check for keywords in text\n",
    "def check_keywords(text, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to perform sentiment analysis using VADER\n",
    "def polarity(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "# Create separate DataFrames based on the presence of keywords\n",
    "df_customer_support = df[df['processed_text'].apply(lambda x: check_keywords(x, keywords['customer_support']))]\n",
    "df_delivery_services = df[df['processed_text'].apply(lambda x: check_keywords(x, keywords['delivery_services']))]\n",
    "df_product_quality = df[df['processed_text'].apply(lambda x: check_keywords(x, keywords['product_quality']))]\n",
    "df_discounts = df[df['processed_text'].apply(lambda x: check_keywords(x, keywords['discounts']))]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "df_customer_support['sentiment'] = df_customer_support['processed_text'].apply(polarity)\n",
    "df_delivery_services['sentiment'] = df_delivery_services['processed_text'].apply(polarity)\n",
    "df_product_quality['sentiment'] = df_product_quality['processed_text'].apply(polarity)\n",
    "df_discounts['sentiment'] = df_discounts['processed_text'].apply(polarity)\n",
    "\n",
    "# Calculate average sentiment scores\n",
    "average_sentiments = {\n",
    "    'customer_support': df_customer_support['sentiment'].mean()*100,\n",
    "    'delivery_services': df_delivery_services['sentiment'].mean()*100,\n",
    "    'product_quality': df_product_quality['sentiment'].mean()*100,\n",
    "    'discounts': df_discounts['sentiment'].mean()*100\n",
    "}\n",
    "print(average_sentiments)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "sentiment_df = pd.DataFrame(list(average_sentiments.items()), columns=['Category', 'Average Sentiment'])\n",
    "\n",
    "# Plot the average sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Category', y='Average Sentiment', data=sentiment_df, palette='viridis')\n",
    "plt.title('Average Sentiment Scores by Category')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.xlabel('Category')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584afa2-d714-4b19-9510-6f3b7d1e6b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
